{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from multiprocess import Pool\n",
    "#from functools import partial\n",
    "import numpy as np\n",
    "#from numba import jit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TODO: loss of least square regression and binary logistic regression\n",
    "'''\n",
    "    pred() takes GBDT/RF outputs, i.e., the \"score\", as its inputs, and returns predictions.\n",
    "    g() is the gradient/1st order derivative, which takes true values \"true\" and scores as input, and returns gradient.\n",
    "    h() is the heassian/2nd order derivative, which takes true values \"true\" and scores as input, and returns hessian.\n",
    "'''\n",
    "class leastsquare(object):\n",
    "    '''Loss class for mse. As for mse, pred function is pred=score.'''\n",
    "    def pred(self,score):\n",
    "        return score\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return 2 * (score - true)\n",
    "\n",
    "    def h(self,true,score):\n",
    "        return 2 * np.ones(score.shape[0])\n",
    "\n",
    "class logistic(object):\n",
    "    '''Loss class for log loss. As for log loss, pred function is logistic transformation.'''\n",
    "    def pred(self,score):\n",
    "        score = 1 / (1 + np.exp(-score))\n",
    "        return score\n",
    "\n",
    "    def g(self,true,score):\n",
    "        return self.pred(score) - true\n",
    "\n",
    "    def h(self,true,score):\n",
    "        sigmoid = self.pred(score)\n",
    "        return sigmoid * (1 - sigmoid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of a node on a tree\n",
    "class TreeNode(object):\n",
    "    '''\n",
    "    Data structure that are used for storing a node on a tree.\n",
    "    \n",
    "    A tree is presented by a set of nested TreeNodes,\n",
    "    with one TreeNode pointing two child TreeNodes,\n",
    "    until a tree leaf is reached.\n",
    "    \n",
    "    A node on a tree can be either a leaf node or a non-leaf node.\n",
    "    '''\n",
    "    \n",
    "    #TODO\n",
    "    #def __init__(self, X, y, depth):\n",
    "    def __init__(\n",
    "        self, \n",
    "        split_feature = None, \n",
    "        split_threshold = None, \n",
    "        depth = None,\n",
    "        weight = None,\n",
    "        left_child = None, \n",
    "        right_child = None):\n",
    "\n",
    "        # store essential information in every tree node\n",
    "        self.split_feature = split_feature \n",
    "        self.split_threshold = split_threshold\n",
    "        self.left_child = left_child\n",
    "        self.right_child = right_child\n",
    "        self.depth = depth\n",
    "        self.weight = weight\n",
    "        \n",
    "        self.is_leaf = (left_child is None) and (right_child is None)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: class of single tree\n",
    "class Tree(object):\n",
    "    '''\n",
    "    Class of a single decision tree in GBDT\n",
    "\n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        max_depth: The maximum depth of the tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf prediction, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of TreeNode, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule,\n",
    "            rf = 0 means we are training a GBDT.\n",
    "    '''\n",
    "    \n",
    "    def __init__(self, n_threads = None, \n",
    "                 max_depth = 3, min_sample_split = 10,\n",
    "                 lamda = 1, gamma = 0, rf = 0):\n",
    "        self.n_threads = n_threads\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.int_member = 0\n",
    "\n",
    "    def fit(self, train, g, h):\n",
    "        '''\n",
    "        train is the training data matrix, and must be numpy array (an n_train x m matrix).\n",
    "        g and h are gradient and hessian respectively.\n",
    "        '''\n",
    "        #TODO\n",
    "        self.tree = self.construct_tree(train, g, h, self.max_depth)\n",
    "        return self\n",
    "\n",
    "    def predict(self,test):\n",
    "        '''\n",
    "        test is the test data matrix, and must be numpy arrays (an n_test x m matrix).\n",
    "        Return predictions (scores) as an array.\n",
    "        '''\n",
    "        #TODO\n",
    "        result = []\n",
    "        n, m = test.shape\n",
    "        for i in range(n):\n",
    "            sample = test[i, :]\n",
    "            cur = self.tree\n",
    "            while not cur.is_leaf:\n",
    "                if cur.left_child is not None and sample[cur.split_feature] < cur.split_threshold:\n",
    "                    cur = cur.left_child\n",
    "                else:\n",
    "                    cur = cur.right_child\n",
    "            result.append(cur.weight)\n",
    "        result = np.array(result)\n",
    "        return result\n",
    "\n",
    "    def construct_tree(self, train, g, h, max_depth):\n",
    "        '''\n",
    "        Tree construction, which is recursively used to grow a tree.\n",
    "        First we should check if we should stop further splitting.\n",
    "        \n",
    "        The stopping conditions include:\n",
    "            1. tree reaches max_depth $d_{max}$\n",
    "            2. The number of sample points at current node is less than min_sample_split, i.e., $n_{min}$\n",
    "            3. gain <= 0\n",
    "        '''\n",
    "        #TODO\n",
    "        (n, m) = train.shape\n",
    "        G, H = np.sum(g), np.sum(h)\n",
    "        weight = -G / (H + self.lamda)\n",
    "        if n < self.min_sample_split or max_depth == 0:\n",
    "            return TreeNode(weight = weight)\n",
    "        best_p, best_t, gain = 0, 0, 0\n",
    "\n",
    "        feature, threshold, gain = self.find_best_decision_rule(train, g, h)\n",
    "        if gain <= 0:\n",
    "            return TreeNode(weight = weight)\n",
    "        \n",
    "        left = train[:, feature] <= threshold\n",
    "        right = train[:, feature] > threshold\n",
    "        left_child = self.construct_tree(train[left, :], g[left], h[left], max_depth - 1)\n",
    "        right_child = self.construct_tree(train[right, :], g[right], h[right], max_depth - 1)\n",
    "        return TreeNode(\n",
    "            split_feature = feature, split_threshold = threshold, \n",
    "            left_child = left_child, right_child = right_child)\n",
    "\n",
    "    def find_best_decision_rule(self, train, g, h):\n",
    "        '''\n",
    "        Return the best decision rule [feature, threshold], i.e., $(p_j, \\tau_j)$ on a node j, \n",
    "        train is the training data assigned to node j\n",
    "        g and h are the corresponding 1st and 2nd derivatives for each data point in train\n",
    "        g and h should be vectors of the same length as the number of data points in train\n",
    "        \n",
    "        for each feature, we find the best threshold by find_threshold(),\n",
    "        a [threshold, best_gain] list is returned for each feature.\n",
    "        Then we select the feature with the largest best_gain,\n",
    "        and return the best decision rule [feature, treshold] together with its gain.\n",
    "        '''\n",
    "        #TODO\n",
    "        (n, m) = train.shape\n",
    "\n",
    "        if self.rf != 0:\n",
    "            idx = np.random.choice(np.arange(m), int(self.rf * m), replace = False)\n",
    "        else:\n",
    "            idx = np.arange(m)\n",
    "       \n",
    "        feature, threshold, best_gain = 0, 0, 0\n",
    "        # multiprocessing version of selecting decision rule; can be applied to RF and GBDT\n",
    "        if self.n_threads is not None and self.n_threads > 1:\n",
    "            with Pool(processes = self.n_threads) as pool:\n",
    "                res = pool.starmap(self.find_threshold, [(g, h, train[:, p],) for p in idx])\n",
    "            #print(res)\n",
    "            for p, (t, gain) in enumerate(res):\n",
    "                if gain > best_gain:\n",
    "                    feature, threshold, best_gain = p, t, gain\n",
    "        else:\n",
    "            for p in idx:\n",
    "                t, gain = self.find_threshold(g, h, train[:, p])\n",
    "                if gain > best_gain:\n",
    "                    feature, threshold, best_gain = p, t, gain\n",
    "        return feature, threshold, best_gain\n",
    "\n",
    "    \n",
    "    def find_threshold(self, g, h, train):\n",
    "        '''\n",
    "        Given a particular feature $p_j$,\n",
    "        return the best split threshold $\\tau_j$ together with the gain that is achieved.\n",
    "        '''\n",
    "        #TODO\n",
    "        #\n",
    "        n = train.shape[0]\n",
    "        threshold, best_gain = 0, -1\n",
    "        idx = sorted(np.arange(n), key = lambda i : train[i])\n",
    "        sorted_train = train[idx]\n",
    "        thresholds = set(sorted_train)\n",
    "        sorted_g, sorted_h = g[idx], h[idx]\n",
    "        G, H = np.sum(sorted_g), np.sum(sorted_h)\n",
    "        GL, GR, HL, HR = 0, G, 0, H\n",
    "        i = 0\n",
    "        for t in list(thresholds)[:-1]:\n",
    "            while i < len(idx) - 1 and sorted_train[i] <= t:\n",
    "                GL += sorted_g[i]\n",
    "                HL += sorted_h[i]\n",
    "                GR -= sorted_g[i]\n",
    "                HR -= sorted_h[i] \n",
    "                i += 1\n",
    "            gain = 0.5 * (\n",
    "                GL ** 2 / (HL + self.lamda) + \\\n",
    "                GR ** 2 / (HR + self.lamda) - \\\n",
    "                G ** 2 / (H + self.lamda)) - self.gamma\n",
    "            if gain > best_gain:\n",
    "                threshold = (sorted_train[i - 1] + sorted_train[i]) / 2\n",
    "                #threshold = t\n",
    "                best_gain = gain\n",
    "        '''for t in range(len(idx) - 1):\n",
    "            GL += sorted_g[t]\n",
    "            HL += sorted_h[t]\n",
    "            GR -= sorted_g[t]\n",
    "            HR -= sorted_h[t] \n",
    "            gain = 0.5 * (\n",
    "                GL ** 2 / (HL + self.lamda) + \\\n",
    "                GR ** 2 / (HR + self.lamda) - \\\n",
    "                G ** 2 / (H + self.lamda)) - self.gamma\n",
    "            if gain > best_gain:\n",
    "                threshold = (sorted_train[t - 1] + sorted_train[t]) / 2\n",
    "                best_gain = gain'''\n",
    "        return [threshold, best_gain]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of Random Forest\n",
    "class RF(object):\n",
    "    '''\n",
    "    Class of Random Forest\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth d_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        rf: rf*m is the size of random subset of features, from which we select the best decision rule.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        rf = 0.99, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.rf = rf\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "\n",
    "        self.loss = leastsquare() if loss == 'mse' else logistic()\n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        (n, m) = train.shape\n",
    "\n",
    "        for i in range(self.num_trees):\n",
    "            idx = np.random.choice(np.arange(n), n, replace = True)\n",
    "            X_sample,y_sample = train[idx, :], target[idx]\n",
    "            pred = np.full(y_sample.shape[0], 0)\n",
    "            #pred = np.full(y_sample.shape[0], np.mean(y_sample))\n",
    "            g, h = self.loss.g(y_sample, pred), self.loss.h(y_sample, pred)\n",
    "\n",
    "            new_tree = Tree(n_threads = self.n_threads, rf = self.rf)\n",
    "            new_tree.fit(X_sample, g, h)\n",
    "            self.trees.append(new_tree)\n",
    "\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "        predictions = np.array([tree.predict(test) for tree in self.trees])\n",
    "        score = np.mean(predictions, axis = 0)\n",
    "        return self.loss.pred(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: class of GBDT\n",
    "class GBDT(object):\n",
    "    '''\n",
    "    Class of gradient boosting decision tree (GBDT)\n",
    "    \n",
    "    Parameters:\n",
    "        n_threads: The number of threads used for fitting and predicting.\n",
    "        loss: Loss function for gradient boosting.\n",
    "            'mse' for regression task and 'log' for classfication task.\n",
    "            A child class of the loss class could be passed to implement customized loss.\n",
    "        max_depth: The maximum depth D_max of a tree.\n",
    "        min_sample_split: The minimum number of samples required to further split a node.\n",
    "        lamda: The regularization coefficient for leaf score, also known as lambda.\n",
    "        gamma: The regularization coefficient for number of tree nodes, also know as gamma.\n",
    "        learning_rate: The learning rate eta of GBDT.\n",
    "        num_trees: Number of trees.\n",
    "    '''\n",
    "    def __init__(self,\n",
    "        n_threads = None, loss = 'mse',\n",
    "        max_depth = 3, min_sample_split = 10, \n",
    "        lamda = 1, gamma = 0,\n",
    "        learning_rate = 0.1, num_trees = 100):\n",
    "        \n",
    "        self.n_threads = n_threads\n",
    "        self.loss = loss\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.lamda = lamda\n",
    "        self.gamma = gamma\n",
    "        self.learning_rate = learning_rate\n",
    "        self.num_trees = num_trees\n",
    "        self.trees = []\n",
    "\n",
    "        self.loss = leastsquare() if loss == 'mse' else logistic()\n",
    "        \n",
    "\n",
    "    def fit(self, train, target):\n",
    "        # train is n x m 2d numpy array\n",
    "        # target is n-dim 1d array\n",
    "        #TODO\n",
    "        pred = np.full(target.shape[0], 0)\n",
    "        g, h = self.learning_rate * self.loss.g(target, pred), self.learning_rate ** 2 *self.loss.h(target, pred)\n",
    "\n",
    "        (n, m) = train.shape\n",
    "        for i in range(self.num_trees):\n",
    "            #idx = np.random.choice(np.arange(n), int(0.99 * n))\n",
    "            #X_sample,y_sample = train[idx, :], target[idx]\n",
    "\n",
    "            new_tree = Tree(n_threads = self.n_threads)\n",
    "            new_tree.fit(train, g, h)\n",
    "            #new_tree.fit(X_sample, g, h)\n",
    "            self.trees.append(new_tree)\n",
    "\n",
    "            pred = self.predict(train)\n",
    "            #pred = self.predict(X_sample)\n",
    "            g, h = self.learning_rate * self.loss.g(target, pred), self.learning_rate ** 2 * self.loss.h(target, pred)\n",
    "            #g, h = self.learning_rate * self.loss.g(y_sample, pred), self.learning_rate ** 2 * self.loss.h(y_sample, pred)\n",
    "        return self\n",
    "\n",
    "    def predict(self, test):\n",
    "        #TODO\n",
    "        predictions = np.array([tree.predict(test) for tree in self.trees])\n",
    "        score = np.sum(predictions, axis = 0)\n",
    "        return self.loss.pred(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Evaluation functions (you can use code from previous homeworks)\n",
    "\n",
    "# RMSE\n",
    "def root_mean_square_error(pred, y):\n",
    "    #TODO\n",
    "    n = pred.shape[0]\n",
    "    error = pred - y\n",
    "    rmse = np.sqrt(np.dot(pred - y, pred - y) / n)\n",
    "    return rmse\n",
    "\n",
    "# precision\n",
    "def accuracy(pred, y):\n",
    "    #TODO\n",
    "    n = pred.shape[0]\n",
    "    return sum(pred == y) / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(506, 13) (506,) (354, 13) (354,) (152, 13) (152,)\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT regression on boston house price dataset\n",
    "\n",
    "# load data\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "data_url = \"http://lib.stat.cmu.edu/datasets/boston\"\n",
    "raw_df = pd.read_csv(data_url, sep=\"\\s+\", skiprows=22, header=None)\n",
    "X = np.hstack([raw_df.values[::2, :], raw_df.values[1::2, :2]])\n",
    "y = raw_df.values[1::2, 2]\n",
    "\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 3.724229667511438 | test rmse: 4.215613270781268\n"
     ]
    }
   ],
   "source": [
    "# fit housing price with Random Forest\n",
    "model = RF(num_trees = 100, )\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred_train = model.predict(X_train)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "\n",
    "pred_test = model.predict(X_test)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "\n",
    "print('train rmse: {} | test rmse: {}'.format(rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 2.153766997451514 | test rmse: 3.785467676530208\n"
     ]
    }
   ],
   "source": [
    "# fit housing price with GBDT\n",
    "model = GBDT(num_trees = 100, learning_rate = 0.001, n_threads = 1)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "pred_train = model.predict(X_train)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "\n",
    "pred_test = model.predict(X_test)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "\n",
    "print('train rmse: {} | test rmse: {}'.format(rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 4.820626531838223 | test rmse: 5.2092175105308245\n"
     ]
    }
   ],
   "source": [
    "# comparison with closed-form linear regression\n",
    "W = np.matmul(\n",
    "    np.linalg.inv(np.matmul(X_train.T, X_train)),\n",
    "    np.matmul(X_train.T, y_train))\n",
    "\n",
    "\n",
    "pred_train = np.matmul(X_train, W)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "\n",
    "pred_test = np.matmul(X_test, W)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "\n",
    "print('train rmse: {} | test rmse: {}'.format(rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 4.822434482543469 | test rmse: 5.186569984437025\n"
     ]
    }
   ],
   "source": [
    "# comparison with closed-form ridge regression\n",
    "lmda = 0.5\n",
    "\n",
    "W = np.matmul(\n",
    "    np.linalg.inv(np.matmul(X_train.T, X_train) + lmda),\n",
    "    np.matmul(X_train.T, y_train))\n",
    "\n",
    "\n",
    "pred_train = np.matmul(X_train, W)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "\n",
    "pred_test = np.matmul(X_test, W)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "\n",
    "print('train rmse: {} | test rmse: {}'.format(rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 4.6364947168982935 | test rmse: 4.924357692077932\n"
     ]
    }
   ],
   "source": [
    "# comparison with sklearn ridge regression\n",
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "rdg = Ridge(alpha = 0.5)\n",
    "rdg.fit(X_train, y_train)\n",
    "\n",
    "pred_train = rdg.predict(X_train)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "\n",
    "pred_test = rdg.predict(X_test)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "\n",
    "print('train rmse: {} | test rmse: {}'.format(rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train rmse: 4.811833896637408 | test rmse: 5.218761727804395\n"
     ]
    }
   ],
   "source": [
    "# comparison with sklearn lasso regression\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "las = Lasso(alpha = 0.5)\n",
    "las.fit(X_train, y_train)\n",
    "\n",
    "pred_train = las.predict(X_train)\n",
    "rmse_train = root_mean_square_error(pred_train, y_train)\n",
    "\n",
    "pred_test = las.predict(X_test)\n",
    "rmse_test = root_mean_square_error(pred_test, y_test)\n",
    "\n",
    "print('train rmse: {} | test rmse: {}'.format(rmse_train, rmse_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: GBDT classification on credit-g dataset\n",
    "\n",
    "# load data\n",
    "from sklearn.datasets import fetch_openml\n",
    "X, y = fetch_openml('credit-g', version=1, return_X_y=True, data_home='credit/', parser='auto')\n",
    "y = np.array(list(map(lambda x: 1 if x == 'good' else 0, y)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "checking_status | values: ['<0', '0<=X<200', 'no checking', '>=200']\n",
      "duration | values: [6, 48, 12, 42, 24, 36, 30, 15, 9, 10, 7, 60, 18, 45, 11, 27, 8, 54, 20, 14, 33, 21, 16, 4, 47, 13, 22, 39, 28, 5, 26, 72, 40]\n",
      "credit_history | values: ['critical/other existing credit', 'existing paid', 'delayed previously', 'no credits/all paid', 'all paid']\n",
      "purpose | values: ['radio/tv', 'education', 'furniture/equipment', 'new car', 'used car', 'business', 'domestic appliance', 'repairs', 'other', 'retraining']\n",
      "credit_amount | values: [1169, 5951, 2096, 7882, 4870, 9055, 2835, 6948, 3059, 5234, 1295, 4308, 1567, 1199, 1403, 1282, 2424, 8072, 12579, 3430, 2134, 2647, 2241, 1804, 2069, 1374, 426, 409, 2415, 6836, 1913, 4020, 5866, 1264, 1474, 4746, 6110, 2100, 1225, 458, 2333, 1158, 6204, 6187, 6143, 1393, 2299, 1352, 7228, 2073, 5965, 1262, 3378, 2225, 783, 6468, 9566, 1961, 6229, 1391, 1537, 1953, 14421, 3181, 5190, 2171, 1007, 1819, 2394, 8133, 730, 1164, 5954, 1977, 1526, 3965, 4771, 9436, 3832, 5943, 1213, 1568, 1755, 2315, 1412, 12612, 2249, 1108, 618, 1409, 797, 3617, 1318, 15945, 2012, 2622, 2337, 7057, 1469, 2323, 932, 1919, 2445, 11938, 6458, 6078, 7721, 1410, 1449, 392, 6260, 7855, 1680, 3578, 7174, 2132, 4281, 2366, 1835, 3868, 1768, 781, 1924, 2121, 701, 639, 1860, 3499, 8487, 6887, 2708, 1984, 10144, 1240, 8613, 766, 2728, 1881, 709, 4795, 3416, 2462, 2288, 3566, 860, 682, 5371, 1582, 1346, 5848, 7758, 6967, 1288, 339, 3512, 1898, 2872, 1055, 7308, 909, 2978, 1131, 1577, 3972, 1935, 950, 763, 2064, 1414, 3414, 7485, 2577, 338, 1963, 571, 9572, 4455, 1647, 3777, 884, 1360, 5129, 1175, 674, 3244, 4591, 3844, 3915, 2108, 3031, 1501, 1382, 951, 2760, 4297, 936, 1168, 5117, 902, 1495, 10623, 1424, 6568, 1413, 3074, 3835, 5293, 1908, 3342, 3104, 3913, 3021, 1364, 625, 1200, 707, 4657, 2613, 10961, 7865, 1478, 3149, 4210, 2507, 2141, 866, 1544, 1823, 14555, 2767, 1291, 2522, 915, 1595, 4605, 1185, 3447, 1258, 717, 1204, 1925, 433, 666, 2251, 2150, 4151, 2030, 7418, 2684, 2149, 3812, 1154, 1657, 1603, 5302, 2748, 1231, 802, 6304, 1533, 8978, 999, 2662, 1402, 12169, 3060, 11998, 2697, 2404, 4611, 1901, 3368, 1574, 1445, 1520, 3878, 10722, 4788, 7582, 1092, 1024, 1076, 9398, 6419, 4796, 7629, 9960, 4675, 1287, 2515, 2745, 672, 3804, 1344, 1038, 10127, 1543, 4811, 727, 1237, 276, 5381, 5511, 3749, 685, 1494, 2746, 708, 4351, 3643, 4249, 1938, 2910, 2659, 1028, 3398, 5801, 1525, 4473, 1068, 6615, 1864, 7408, 11590, 4110, 3384, 2101, 1275, 4169, 1521, 5743, 3599, 3213, 4439, 3949, 1459, 882, 3758, 1743, 1136, 1236, 959, 3229, 6199, 1246, 2331, 4463, 776, 2406, 1239, 3399, 2247, 1766, 2473, 1542, 3850, 3650, 3446, 3001, 3079, 6070, 2146, 13756, 14782, 7685, 2320, 846, 14318, 362, 2212, 12976, 1283, 1330, 4272, 2238, 1126, 7374, 2326, 1820, 983, 3249, 1957, 11760, 2578, 2348, 1223, 1516, 1473, 1887, 8648, 2899, 2039, 2197, 1053, 3235, 939, 1967, 7253, 2292, 1597, 1381, 5842, 2579, 8471, 2782, 1042, 3186, 2028, 958, 1591, 2762, 2779, 2743, 1149, 1313, 1190, 3448, 11328, 1872, 2058, 2136, 1484, 660, 3394, 609, 1884, 1620, 2629, 719, 5096, 1244, 1842, 2576, 1512, 11054, 518, 2759, 2670, 4817, 2679, 3905, 3386, 343, 4594, 3620, 1721, 3017, 754, 1950, 2924, 1659, 7238, 2764, 4679, 3092, 448, 654, 1238, 1245, 3114, 2569, 5152, 1037, 3573, 1201, 3622, 960, 1163, 1209, 3077, 3757, 1418, 3518, 1934, 8318, 368, 2122, 2996, 9034, 1585, 1301, 1323, 3123, 5493, 1216, 1207, 1309, 2360, 6850, 8588, 759, 4686, 2687, 585, 2255, 1361, 7127, 1203, 700, 5507, 3190, 7119, 3488, 1113, 7966, 1532, 1503, 2302, 662, 2273, 2631, 1311, 3105, 2319, 3612, 7763, 3049, 1534, 2032, 6350, 2864, 1255, 1333, 2022, 1552, 626, 8858, 996, 1750, 6999, 1995, 1331, 2278, 5003, 3552, 1928, 2964, 1546, 683, 12389, 4712, 1553, 1372, 3979, 6758, 3234, 5433, 806, 1082, 2788, 2930, 1927, 2820, 937, 1056, 3124, 1388, 2384, 2133, 2799, 1289, 1217, 2246, 385, 1965, 1572, 2718, 1358, 931, 1442, 4241, 2775, 3863, 2329, 918, 1837, 3349, 2828, 4526, 2671, 2051, 1300, 741, 3357, 3632, 1808, 12204, 9157, 3676, 3441, 640, 3652, 1530, 3914, 1858, 2600, 1979, 2116, 1437, 4042, 3660, 1444, 1980, 1355, 1376, 15653, 1493, 4370, 750, 1308, 4623, 1851, 1880, 7980, 4583, 1386, 947, 684, 7476, 1922, 2303, 8086, 2346, 3973, 888, 10222, 4221, 6361, 1297, 900, 1050, 1047, 6314, 3496, 3609, 4843, 4139, 5742, 10366, 2080, 2580, 4530, 5150, 5595, 1453, 1538, 2279, 5103, 9857, 6527, 1347, 2862, 2753, 3651, 975, 2896, 4716, 2284, 1103, 926, 1800, 1905, 1123, 6331, 1377, 2503, 2528, 5324, 6560, 2969, 1206, 2118, 629, 1198, 2476, 1138, 14027, 7596, 1505, 3148, 6148, 1337, 1228, 790, 2570, 250, 1316, 1882, 6416, 6403, 1987, 760, 2603, 3380, 3990, 11560, 4380, 6761, 4280, 2325, 1048, 3160, 2483, 14179, 1797, 2511, 1274, 5248, 3029, 428, 976, 841, 5771, 1555, 1285, 1299, 1271, 691, 5045, 2124, 2214, 12680, 2463, 1155, 3108, 2901, 1655, 2812, 8065, 3275, 2223, 1480, 1371, 3535, 3509, 5711, 3872, 4933, 1940, 836, 1941, 2675, 2751, 6224, 5998, 1188, 6313, 1221, 2892, 3062, 2301, 7511, 1549, 1795, 7472, 9271, 590, 930, 9283, 1778, 907, 484, 9629, 3051, 3931, 7432, 1338, 1554, 15857, 1345, 1101, 3016, 2712, 731, 3780, 1602, 3966, 4165, 8335, 6681, 2375, 11816, 5084, 2327, 886, 601, 2957, 2611, 5179, 2993, 1943, 1559, 3422, 3976, 1249, 2235, 1471, 10875, 894, 3343, 3959, 3577, 5804, 2169, 2439, 2210, 2221, 2389, 3331, 7409, 652, 7678, 1343, 874, 3590, 1322, 3595, 1422, 6742, 7814, 9277, 2181, 1098, 4057, 795, 2825, 15672, 6614, 7824, 2442, 1829, 5800, 8947, 2606, 1592, 2186, 4153, 2625, 3485, 10477, 1278, 1107, 3763, 3711, 3594, 3195, 4454, 4736, 2991, 2142, 3161, 18424, 2848, 14896, 2359, 3345, 1817, 12749, 1366, 2002, 6872, 697, 1049, 10297, 1867, 1747, 1670, 1224, 522, 1498, 745, 2063, 6288, 6842, 3527, 929, 1455, 1845, 8358, 2859, 3621, 2145, 4113, 10974, 1893, 3656, 4006, 3069, 1740, 2353, 3556, 2397, 454, 1715, 2520, 3568, 7166, 3939, 1514, 7393, 1193, 7297, 2831, 753, 2427, 2538, 8386, 4844, 2923, 8229, 1433, 6289, 6579, 3565, 1569, 1936, 2390, 1736, 3857, 804, 4576]\n",
      "savings_status | values: ['no known savings', '<100', '500<=X<1000', '>=1000', '100<=X<500']\n",
      "employment | values: ['>=7', '1<=X<4', '4<=X<7', 'unemployed', '<1']\n",
      "installment_commitment | values: [4, 2, 3, 1]\n",
      "personal_status | values: ['male single', 'female div/dep/mar', 'male div/sep', 'male mar/wid']\n",
      "other_parties | values: ['none', 'guarantor', 'co applicant']\n",
      "residence_since | values: [4, 2, 3, 1]\n",
      "property_magnitude | values: ['real estate', 'life insurance', 'no known property', 'car']\n",
      "age | values: [67, 22, 49, 45, 53, 35, 61, 28, 25, 24, 60, 32, 44, 31, 48, 26, 36, 39, 42, 34, 63, 27, 30, 57, 33, 37, 58, 23, 29, 52, 50, 46, 51, 41, 40, 66, 47, 56, 54, 20, 21, 38, 70, 65, 74, 68, 43, 55, 64, 75, 19, 62, 59]\n",
      "other_payment_plans | values: ['none', 'bank', 'stores']\n",
      "housing | values: ['own', 'for free', 'rent']\n",
      "existing_credits | values: [2, 1, 3, 4]\n",
      "job | values: ['skilled', 'unskilled resident', 'high qualif/self emp/mgmt', 'unemp/unskilled non res']\n",
      "num_dependents | values: [1, 2]\n",
      "own_telephone | values: ['yes', 'none']\n",
      "foreign_worker | values: ['yes', 'no']\n",
      "y | [1 0 1 1 0 1 1 1 1 0 0 0 1 0 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 1\n",
      " 0 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 0 1 1 1 1 1\n",
      " 0 1 0 1 1 1 0 1 1 1 1 1 1 0 1 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1\n",
      " 1 1 0 1 1 0 1 0 1 0 1 1 1 0 1 1 0 1 0 1 0 1 1 1 1 1 0 1 1 1 1 1 0 1 1 1 1\n",
      " 1 1 1 1 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 1 0 0 1 1 1 1 0 0 0 1 0\n",
      " 1 0 1 0 1 0 0 0 1 0 0 1 0 1 0 1 1 1 0 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 0 0 0 1 0 1 1 1 1 0 0 0 1 1 0 1 0 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 1\n",
      " 1 1 1 1 1 1 0 1 1 0 1 1 1 1 0 0 1 1 1 0 1 1 1 1 1 1 1 1 1 1 0 1 0 1 1 1 0\n",
      " 1 1 1 1 1 0 0 1 0 1 1 0 0 1 1 1 1 0 1 0 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 0 0\n",
      " 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 0 1 0 1 0 1 0 1 0 1 0 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 1 0 0 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 1\n",
      " 1 1 0 1 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 1 0 1 0 1 1 1 0 1 1 1 0 1 1 1 0\n",
      " 0 1 0 1 1 0 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 1 0 0 1 1 1 1 1\n",
      " 1 1 1 1 0 1 1 1 1 1 0 1 1 1 0 0 1 1 1 0 1 1 0 0 0 1 0 1 1 0 1 1 1 1 1 1 0\n",
      " 1 1 1 0 0 1 1 1 1 1 0 1 1 0 1 1 1 0 1 1 0 1 0 1 0 0 1 0 1 1 0 1 1 1 0 1 1\n",
      " 0 0 0 0 0 1 0 1 0 1 1 0 1 1 0 0 1 1 1 1 1 1 1 0 1 0 1 1 0 1 0 1 1 0 0 1 1\n",
      " 1 0 0 0 0 0 0 1 1 0 0 0 1 1 1 0 1 1 0 0 1 1 0 1 1 1 0 1 1 0 0 1 0 1 1 0 1\n",
      " 1 1 0 1 0 0 1 1 1 1 0 0 1 0 1 1 0 1 0 0 0 1 0 0 0 1 1 0 1 1 1 1 0 1 1 1 1\n",
      " 1 1 0 1 1 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 1\n",
      " 1 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 0 0 0 1 0 1 1 0 0 1 1 0 1 1 1 1 0 1 1 0\n",
      " 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0 1 0 1 0 1 0 1 0 1 1 0 1 1 1 1 0 1 1 1 0 1\n",
      " 1 1 1 0 1 1 0 1 1 1 1 0 0 0 1 1 1 1 1 0 1 1 1 1 1 1 1 1 0 1 1 1 0 1 1 0 0\n",
      " 0 1 1 1 1 0 1 1 0 1 1 1 0 0 0 1 1 0 0 1 0 0 1 1 1 1 0 1 0 1 1 1 0 1 1 0 0\n",
      " 1 1 0 1 1 1 1 0 1 1 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 0 1 1 1 1 1 0 0 1 0\n",
      " 1 1 1 1 1 1 1 1 1 1 1 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 0 0 1 0 0 0 1 1 0 1 0\n",
      " 0 1 0 1 1 1 0 1 1 1 0 0 1 0 1 1 1 1 1 1 1 0 1 0 0 1 0 0 0 1 1 1 1 0 1 1 1\n",
      " 1 0 1 1 0 1 1 1 1 1 0 0 1 1 1 1 0 0 0 0 1 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0\n",
      " 1]\n"
     ]
    }
   ],
   "source": [
    "# Inspect the dataset\n",
    "#print(X.columns)\n",
    "\n",
    "def unique(lst):\n",
    "    res = []\n",
    "    for each in lst:\n",
    "        if each not in res:\n",
    "            res.append(each)\n",
    "    return res\n",
    "\n",
    "for col in X.columns:\n",
    "    #break\n",
    "    print('{} | values: {}'.format(col, unique(X[col].tolist())))\n",
    "\n",
    "print('y | {}'.format(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1000, 20) (1000,) (700, 20) (700,) (300, 20) (300,)\n"
     ]
    }
   ],
   "source": [
    "# Preprocess the dataset\n",
    "non_numeric = X.select_dtypes(exclude = 'number').columns\n",
    "for col in non_numeric:\n",
    "    unique = X[col].unique()\n",
    "    mapping = {each : i / len(unique) - 0.5 for i, each in enumerate(unique)}\n",
    "    X[col] = X[col].map(mapping)\n",
    "\n",
    "X = X.values\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.7771428571428571 | test accuracy: 0.7366666666666667\n"
     ]
    }
   ],
   "source": [
    "# classify breast cancer with RF\n",
    "model = RF(num_trees = 100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "threshold = 0.5\n",
    "pred_train = model.predict(X_train) > threshold\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "\n",
    "pred_test = model.predict(X_test) > threshold\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "\n",
    "print('train accuracy: {} | test accuracy: {}'.format(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.7757142857142857 | test accuracy: 0.7366666666666667\n"
     ]
    }
   ],
   "source": [
    "# classify breast cancer with GBDT\n",
    "model = GBDT(num_trees = 80, learning_rate = 7.5e-5)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "threshold = 0.5\n",
    "pred_train = model.predict(X_train) > threshold\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "\n",
    "pred_test = model.predict(X_test) > threshold\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "\n",
    "print('train accuracy: {} | test accuracy: {}'.format(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.7328571428571429 | test accuracy: 0.71\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zephyr/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Comparison with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "pred_train = logreg.predict(X_train)\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "\n",
    "pred_test = logreg.predict(X_test)\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "\n",
    "print('train accuracy: {} | test accuracy: {}'.format(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(569, 30) (569,) (398, 30) (398,) (171, 30) (171,)\n",
      "[[1.026e+01 1.658e+01 6.585e+01 3.208e+02 8.877e-02 8.066e-02 4.358e-02\n",
      "  2.438e-02 1.669e-01 6.714e-02 1.144e-01 1.023e+00 9.887e-01 7.326e+00\n",
      "  1.027e-02 3.084e-02 2.613e-02 1.097e-02 2.277e-02 5.890e-03 1.083e+01\n",
      "  2.204e+01 7.108e+01 3.574e+02 1.461e-01 2.246e-01 1.783e-01 8.333e-02\n",
      "  2.691e-01 9.479e-02]\n",
      " [1.298e+01 1.935e+01 8.452e+01 5.140e+02 9.579e-02 1.125e-01 7.107e-02\n",
      "  2.950e-02 1.761e-01 6.540e-02 2.684e-01 5.664e-01 2.465e+00 2.065e+01\n",
      "  5.727e-03 3.255e-02 4.393e-02 9.811e-03 2.751e-02 4.572e-03 1.442e+01\n",
      "  2.195e+01 9.921e+01 6.343e+02 1.288e-01 3.253e-01 3.439e-01 9.858e-02\n",
      "  3.596e-01 9.166e-02]\n",
      " [1.469e+01 1.398e+01 9.822e+01 6.561e+02 1.031e-01 1.836e-01 1.450e-01\n",
      "  6.300e-02 2.086e-01 7.406e-02 5.462e-01 1.511e+00 4.795e+00 4.945e+01\n",
      "  9.976e-03 5.244e-02 5.278e-02 1.580e-02 2.653e-02 5.444e-03 1.646e+01\n",
      "  1.834e+01 1.141e+02 8.092e+02 1.312e-01 3.635e-01 3.219e-01 1.108e-01\n",
      "  2.827e-01 9.208e-02]] [1 1 1]\n"
     ]
    }
   ],
   "source": [
    "# TODO: GBDT classification on breast cancer dataset\n",
    "\n",
    "# load data\n",
    "from sklearn import datasets\n",
    "breast_cancer = datasets.load_breast_cancer()\n",
    "X = breast_cancer.data\n",
    "y = breast_cancer.target\n",
    "\n",
    "# train-test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=8)\n",
    "print(X.shape, y.shape, X_train.shape, y_train.shape, X_test.shape, y_test.shape)\n",
    "print(X_train[:3], y_train[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9849246231155779 | test accuracy: 0.9473684210526315\n"
     ]
    }
   ],
   "source": [
    "# classify breast cancer with RF\n",
    "model = RF(num_trees = 100)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "threshold = 0.5\n",
    "pred_train = model.predict(X_train) > threshold\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "\n",
    "pred_test = model.predict(X_test) > threshold\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "\n",
    "print('train accuracy: {} | test accuracy: {}'.format(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9949748743718593 | test accuracy: 0.9590643274853801\n"
     ]
    }
   ],
   "source": [
    "# classify breast cancer with GBDT\n",
    "model = GBDT(num_trees = 100, learning_rate = 7.5e-4)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "threshold = 0.5\n",
    "pred_train = model.predict(X_train) > threshold\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "\n",
    "pred_test = model.predict(X_test) > threshold\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "\n",
    "print('train accuracy: {} | test accuracy: {}'.format(acc_train, acc_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy: 0.9547738693467337 | test accuracy: 0.9473684210526315\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/zephyr/opt/anaconda3/envs/myenv/lib/python3.10/site-packages/sklearn/linear_model/_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "# Comparison with logistic regression\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg = LogisticRegression(random_state=16)\n",
    "\n",
    "logreg.fit(X_train, y_train)\n",
    "\n",
    "pred_train = logreg.predict(X_train)\n",
    "acc_train = accuracy(pred_train, y_train)\n",
    "\n",
    "pred_test = logreg.predict(X_test)\n",
    "acc_test = accuracy(pred_test, y_test)\n",
    "\n",
    "print('train accuracy: {} | test accuracy: {}'.format(acc_train, acc_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.9 ('myenv')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "cf1ee045094a7edcaf157dc10598d3c573f589dd9a2b2fe8dfda33b9ea61c786"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
